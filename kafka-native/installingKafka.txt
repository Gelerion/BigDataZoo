STANDALONE SERVER

The following example installs Zookeeper with a basic configuration in /usr/local/zookeeper, storing its data
in /var/lib/zookeeper:
    # tar -zxf zookeeper-3.4.6.tar.gz
    # mv zookeeper-3.4.6 /usr/local/zookeeper
    # mkdir -p /var/lib/zookeeper
    # cat > /usr/local/zookeeper/conf/zoo.cfg << EOF
    > tickTime=2000
    > dataDir=/var/lib/zookeeper
    > clientPort=2181
    > EOF
    # export JAVA_HOME=/usr/java/jdk1.8.0_51
    # /usr/local/zookeeper/bin/zkServer.sh start
    JMX enabled by default
    Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
    Starting zookeeper ... STARTED

ZOOKEEPER ENSEMBLE

A Zookeeper cluster is called an ensemble. Due to the algorithm used, it is recommended that ensembles contain an odd
number of servers (e.g., 3, 5, etc.) as a majority of ensemble members (a quorum) must be working in order for Zookeeper
to respond to requests.

To configure Zookeeper servers in an ensemble, they must have a common configuration that lists all servers, and each
server needs a myid file in the data directory that specifies the ID number of the server. If the hostnames of the servers
in the ensemble are zoo1.example.com, zoo2.example.com, and zoo3.example.com, the configuration file might look like this:

tickTime=2000
dataDir=/var/lib/zookeeper
clientPort=2181
initLimit=20
syncLimit=5
server.1=zoo1.example.com:2888:3888
server.2=zoo2.example.com:2888:3888
server.3=zoo3.example.com:2888:3888

In this configuration, the initLimit is the amount of time to allow followers to connect with a leader. The syncLimit
value limits how out-of-sync followers can be with the leader. Both values are a number of tickTime units, which makes
the initLimit 20 * 2000 ms, or 40 seconds. The configuration also lists each server in the ensemble. The servers are
specified in the format server.X=hostname:peerPort:leaderPort

 - X The ID number of the server. This must be an integer, but it does not need to be zero-based or sequential.
 - hostname The hostname or IP address of the server.
 - peerPort The TCP port over which servers in the ensemble communicate with each other.
 - leaderPort The TCP port over which leader election is performed.

Clients only need to be able to connect to the ensemble over the clientPort, but the members of the ensemble must be
able to communicate with each other over all three ports.

In addition to the shared configuration file, each server must have a file in the dataDir directory with the name myid.
This file must contain the ID number of the server, which must match the configuration file. Once these steps are complete,
the servers will start up and communicate with each other in an ensemble.

Installing a Kafka Broker
Once Java and Zookeeper are configured, you are ready to install Apache Kafka. The current release of Kafka can be
downloaded at http://kafka.apache.org/downloads.html. At press time, that version is 0.9.0.1 running
under Scala version 2.11.0.

The following example installs Kafka in /usr/local/kafka, configured to use the Zookeeper server started previously and
to store the message log segments stored in /tmp/kafka-logs:

    # tar -zxf kafka_2.11-0.9.0.1.tgz
    # mv kafka_2.11-0.9.0.1 /usr/local/kafka
    # mkdir /tmp/kafka-logs
    # export JAVA_HOME=/usr/java/jdk1.8.0_51
    # /usr/local/kafka/bin/kafka-server-start.sh -daemon
    /usr/local/kafka/config/server.properties
    #

Broker Configuration
The example configuration provided with the Kafka distribution is sufficient to run a standalone server as a proof of
concept, but it will not be sufficient for most installations.

General Broker
There are several broker configurations that should be reviewed when deploying Kafka for any environment other than a
standalone broker on a single server. These parameters deal with the basic configuration of the broker, and most of them
must be changed to run properly in a cluster with other brokers.

 - BROKER.ID
   Every Kafka broker must have an integer identifier, which is set using the broker.id configuration. By default, this
   integer is set to 0, but it can be any value. The most important thing is that the integer must be unique within a single
   Kafka cluster. The selection of this number is arbitrary, and it can be moved between brokers if necessary for maintenance tasks.

 - PORT
   The example configuration file starts Kafka with a listener on TCP port 9092. This can be set to any available port by
   changing the port configuration parameter. Keep in mind that if a port lower than 1024 is chosen, Kafka must be started
   as root. Running Kafka as root is not a recommended configuration.

 - ZOOKEEPER.CONNECT
   The location of the Zookeeper used for storing the broker metadata is set using the zookeeper.connect configuration parameter.
   The example configuration uses a Zookeeper running on port 2181 on the local host, which is specified as localhost:2181.
   The format for this parameter is a semicolon-separated list of hostname:port/path strings, which include:
        * hostname, the hostname or IP address of the Zookeeper server.
        * port, the client port number for the server.
        * /path, an optional Zookeeper path to use as a chroot environment for the Kafka cluster. If it is omitted, the root path is used.
   ! It is generally considered to be good practice to use a chroot path for the Kafka cluster. This allows the Zookeeper
     ensemble to be shared with other applications, including other Kafka clusters, without a conflict. It is also best to
     specify multiple Zookeeper servers (which are all part of the same ensemble) in this configuration. This allows the
     Kafka broker to connect to another member of the Zookeeper ensemble in the event of server failure.

 - LOG.DIRS
   Kafka persists all messages to disk, and these log segments are stored in the directories specified in the log.dirs configuration.
   This is a comma-separated list of paths on the local system. If more than one path is specified, the broker will store partitions
   on them in a “least-used” fashion with one partition’s log segments stored within the same path. Note that the broker will place
   a new partition in the path that has the least number of partitions currently stored in it, not the least amount of disk space used

 - NUM.RECOVERY.THREADS.PER.DATA.DIR
   Kafka uses a configurable pool of threads for handling log segments. Currently, this thread pool is used:
        * When starting normally, to open each partition’s log segments
        * When starting after a failure, to check and truncate each partition’s log segments
        * When shutting down, to cleanly close log segments
   By default, only one thread per log directory is used. As these threads are only used during startup and shutdown, it is reasonable
   to set a larger number of threads in order to parallelize operations. Specifically, when recovering from an unclean shutdown,
   this can mean the difference of several hours when restarting a broker with a large number of partitions! When setting this
   parameter, remember that the number configured is per log directory specified with log.dirs. This means that if
   num.recovery.threads.per.data.dir is set to 8, and there are 3 paths specified in log.dirs, this is a total of 24 threads.

 - AUTO.CREATE.TOPICS.ENABLE
   The default Kafka configuration specifies that the broker should automatically create a topic under the following circumstances:
        * When a producer starts writing messages to the topic
        * When a consumer starts reading messages from the topic
        * When any client requests metadata for the topic
   In many situations, this can be undesirable behavior, especially as there is no way to validate the existence of a topic through
   the Kafka protocol without causing it to be created. If you are managing topic creation explicitly, whether manually or through
   a provisioning system, you can set the auto.create.topics.enable configuration to false.

  === Topic Defaults ===

 - NUM.PARTITIONS
   The num.partitions parameter determines how many partitions a new topic is created with, primarily when automatic topic creation
   is enabled (which is the default setting). This parameter defaults to one partition. Keep in mind that the number of partitions
   for a topic can only be increased, never decreased. This means that if a topic needs to have fewer partitions than num.partitions,
   care will need to be taken to manually create the topic
        * If you are sending messages to partitions based on keys, adding partitions later can be very challenging, so calculate
          throughput based on your expected future usage, not the current usage.
   If you have some estimate regarding the target throughput of the topic and the expected throughput of the consumers, you can divide
   the target throughput by the expected consumer throughput and derive the number of partitions this way. So if I want to be able to
   write and read 1 GB/sec from a topic, and I know each consumer can only process 50 MB/s, then I know I need at least 20 partitions.
   This way, I can have 20 consumers reading from the topic and achieve 1 GB/sec

 - LOG.RETENTION.MS
   The most common configuration for how long Kafka will retain messages is by time. The default is specified in the configuration
   file using the log.retention.hours parameter, and it is set to 168 hours, or one week. However, there are two other parameters
   allowed, log.retention.minutes and log.retention.ms. All three of these specify the same configuration—the amount of time after
   which messages may be deleted—but the recommended parameter to use is log.retention.ms, as the smaller unit size will take precedence
   if more than one is specified.

 - LOG.RETENTION.BYTES
   Another way to expire messages is based on the total number of bytes of messages retained. This value is set using the log.retention.bytes
   parameter, and it is applied per-partition. This means that if you have a topic with 8 partitions, and log.retention.bytes is set to 1 GB,
   the amount of data retained for the topic will be 8 GB at most.

 - LOG.SEGMENT.BYTES
   The log-retention settings operate on log segments, not individual messages.
   The log-retention settings previously mentioned operate on log segments, not individual messages. As messages are produced to the Kafka broker,
   they are appended to the current log segment for the partition. Once the log segment has reached the size specified by the log.segment.bytes
   parameter, which defaults to 1 GB, the log segment is closed and a new one is opened. Once a log segment has been closed, it can be considered
   for expiration. A smaller log-segment size means that files must be closed and allocated more often, which reduces the overall efficiency
   of disk writes.
   ! Adjusting the size of the log segments can be important if topics have a low produce rate. For example, if a topic receives only 100 megabytes
     per day of messages, and log.segment.bytes is set to the default, it will take 10 days to fill one segment. As messages cannot be expired until
     the log segment is closed, if log.retention.ms is set to 604800000 (1 week), there will actually be up to 17 days of messages retained until
     the closed log segment expires. This is because once the log segment is closed with the current 10 days of messages, that log segment must be
     retained for 7 days before it expires based on the time policy (as the segment cannot be removed until the last message in the segment can be expired).

 - LOG.SEGMENT.MS
   Another way to control when log segments are closed is by using the log.segment.ms parameter, which specifies the amount of time after
   which a log segment should be closed. As with the log.retention.bytes and log.retention.ms parameters, log.segment.bytes and
   log.segment.ms are not mutually exclusive properties. Kafka will close a log segment either when the size limit is reached or
   when the time limit is reached, whichever comes first. By default, there is no setting for log.segment.ms, which results in only
   closing log segments by size.
   ! When using a time-based log segment limit, it is important to consider the impact on disk performance when multiple log segments
     are closed simultaneously. This can happen when there are many partitions that never reach the size limit for log segments, as
     the clock for the time limit will start when the broker starts and will always execute at the same time for these low-volume partitions.